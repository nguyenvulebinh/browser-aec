<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Demo: Echo Cancellation</title>
    <style>
        body { font-family: sans-serif; background-color: #f4f4f9; color: #333; line-height: 1.6; padding: 20px; }
        .container { max-width: 800px; margin: 0 auto; background: white; padding: 25px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1, h2 { text-align: center; color: #444; }
        .controls { text-align: center; margin-bottom: 20px; }
        button { font-size: 16px; padding: 10px 20px; margin: 5px; cursor: pointer; border: none; border-radius: 5px; background-color: #007bff; color: white; transition: background-color 0.2s; }
        button:disabled { background-color: #ccc; cursor: not-allowed; }
        button:hover:not(:disabled) { background-color: #0056b3; }
        .file-input-container { text-align: center; margin: 15px 0; }
        input[type="file"] { margin: 10px; padding: 8px; border: 2px solid #007bff; border-radius: 4px; }
        .visualizers { display: flex; justify-content: space-around; flex-wrap: wrap; margin-top: 20px; }
        .visualizer-box { text-align: center; margin: 10px; }
        canvas { background-color: #2c3e50; border-radius: 4px; }
        .instructions { background-color: #eaf2ff; border-left: 5px solid #007bff; padding: 15px; margin-top: 20px; border-radius: 4px; }
        .recorded-audio { text-align: center; margin-top: 20px; padding: 15px; background-color: #f0f8ff; border-radius: 4px; }
        p, li { font-size: 16px; }
        #status { text-align: center; font-weight: bold; margin-top: 15px; height: 20px; }
        audio { margin: 10px; }
    </style>
</head>
<body>

<div class="container">
    <h1>Echo Cancellation</h1>
    <p class="instructions">
        <strong>How This Demo Works:</strong><br><br>
        
        This demonstration showcases browser-based <strong>Acoustic Echo Cancellation (AEC)</strong> using a WebRTC loopback technique. Here's the technical breakdown:
        
        <br><br><strong>The Problem:</strong><br>
        When a voice assistant plays audio (like TTS responses), that audio can be picked up by the microphone and interfere with voice recognition. Browsers have built-in AEC, but it only works with audio that arrives through WebRTC connections - not locally generated audio.
        
        <br><br><strong>getUserMedia vs WebRTC - The Missing Link:</strong><br>
        <strong>getUserMedia's Role:</strong> Provides the microphone stream with AEC capability built-in. When you set <code>echoCancellation: true</code>, the browser applies acoustic echo cancellation to the microphone audio stream.<br><br>
        
        <strong>The Key Insight:</strong> Browser AEC only cancels audio it "knows about" - specifically audio that comes through WebRTC peer connections.<br><br>
        
        <strong>Without WebRTC:</strong><br>
        &nbsp;&nbsp;&nbsp;Locally Generated Audio → Speakers → Microphone → getUserMedia<br>
        &nbsp;&nbsp;&nbsp;Browser AEC: "I don't know about this audio, so I won't cancel it"<br><br>
        
        <strong>With WebRTC Loopback:</strong><br>
        &nbsp;&nbsp;&nbsp;Locally Generated Audio → WebRTC → Speakers → Microphone → getUserMedia<br>
        &nbsp;&nbsp;&nbsp;Browser AEC: "This is 'remote participant' audio, I should cancel it!"
        
        <br><br><strong>The Solution - WebRTC Loopback:</strong><br>
        1. <strong>Audio Generation:</strong> Audio is created using the Web Audio API (either uploaded files or the TTS sample)<br>
        2. <strong>WebRTC Routing:</strong> The audio is routed through a peer connection loopback:<br>
        &nbsp;&nbsp;&nbsp;• Audio → WebRTC peer connection → "WebRTC Echo Audio" → Your speakers<br>
        &nbsp;&nbsp;&nbsp;• Same audio stream → AEC system (as reference signal)<br>
        3. <strong>AEC:</strong> The browser's AEC now "knows" about the audio and automatically removes it from the microphone input
        
        <br><br><strong>The Loopback Trick:</strong><br>
        We create two peer connections (pc1 ↔ pc2) talking to each other. The TTS audio is sent through this loopback, making the browser treat our locally generated audio as if it were a "remote participant" in a video call. This bridges locally generated audio to the browser's built-in AEC system.
        
        <br><br><strong>Real-time Visualization:</strong><br>
        • <strong>Microphone Input:</strong> Shows your voice and background audio after AEC processing<br>
        • <strong>Audio to be Cancelled:</strong> Shows the reference audio that should be removed<br>
        • <strong>WebRTC Echo Audio:</strong> The audio stream that teaches AEC what to cancel
        
        <br><br><strong>AEC Toggle:</strong><br>
        Toggle echo cancellation ON/OFF to see the dramatic difference. With AEC OFF, played audio bleeds into the microphone. With AEC ON, it's completely removed while preserving your voice.
        
        <br><br><strong>Recording:</strong><br>
        The demo continuously records your microphone to demonstrate how well AEC works in practice. When you stop, you can hear the processed audio that a voice assistant would actually receive.
        
        <br><br>This technique is used by real voice assistants to prevent their own speech from triggering unwanted activations or interfering with user commands.
    </p>

    <div class="file-input-container">
        <button id="loadSampleButton">Load TTS Sample</button>
        <div style="margin: 10px 0;">OR</div>
        <input type="file" id="audioFileInput" accept="audio/*">
        <div id="fileStatus"></div>
    </div>

    <div class="controls">
        <div style="margin-bottom: 15px;">
            <label style="font-size: 16px; font-weight: bold;">
                <input type="checkbox" id="aecToggle" checked style="margin-right: 8px; transform: scale(1.2);">
                Echo Cancellation (AEC) 
                <span id="aecStatus" style="color: green;">ON</span>
            </label>
        </div>
        <button id="startButton">Start Demo</button>
        <button id="playButton" disabled>Play Audio</button>
        <button id="stopButton" disabled>Stop Demo</button>
    </div>

    <div id="status"></div>

    <div style="text-align: center; margin: 10px 0;">
        <label style="font-weight: bold;">WebRTC Echo Audio (for AEC): <span id="webrtcStatus" style="color: orange;">Connecting...</span></label><br>
        <audio id="remoteAudio" autoplay playsinline controls style="margin: 10px 0;"></audio>
    </div>

    <div class="visualizers">
        <div class="visualizer-box">
            <h2 id="micVisualizerTitle">Microphone Input (AEC ON)</h2>
            <canvas id="micCanvas" width="300" height="150"></canvas>
        </div>
        <div class="visualizer-box">
            <h2>Audio to be Cancelled</h2>
            <canvas id="ttsCanvas" width="300" height="150"></canvas>
        </div>
    </div>

    <div class="recorded-audio" id="recordedAudioContainer" style="display:none;">
        <h2>Recorded Microphone Audio (After AEC)</h2>
        <div id="safariWarning" style="display:none; background-color: #fff3cd; border: 1px solid #ffecb5; padding: 10px; margin: 10px 0; border-radius: 4px; color: #856404;">
            <strong>Safari Note:</strong> Safari may not play back recorded audio directly. If playback doesn't work, use the download button to save the audio file and play it in another app.
        </div>
        <audio id="recordedAudio" controls></audio>
        <br>
        <button id="downloadButton">Download Recording</button>
    </div>
</div>

<script>
    // =============================================================================
    // DOM ELEMENT REFERENCES
    // =============================================================================
    const startButton = document.getElementById('startButton');
    const playButton = document.getElementById('playButton');
    const stopButton = document.getElementById('stopButton');
    const audioFileInput = document.getElementById('audioFileInput');
    const fileStatus = document.getElementById('fileStatus');
    const remoteAudio = document.getElementById('remoteAudio');
    const statusDiv = document.getElementById('status');
    const micCanvas = document.getElementById('micCanvas');
    const ttsCanvas = document.getElementById('ttsCanvas');
    const recordedAudioContainer = document.getElementById('recordedAudioContainer');
    const recordedAudio = document.getElementById('recordedAudio');
    const downloadButton = document.getElementById('downloadButton');
    const loadSampleButton = document.getElementById('loadSampleButton');
    const aecToggle = document.getElementById('aecToggle');
    const aecStatus = document.getElementById('aecStatus');
    const micVisualizerTitle = document.getElementById('micVisualizerTitle');
    const webrtcStatus = document.getElementById('webrtcStatus');
    const safariWarning = document.getElementById('safariWarning');

    // =============================================================================
    // GLOBAL VARIABLES
    // =============================================================================
    let audioContext;           // Web Audio API context
    let micStream;             // Microphone MediaStream with AEC enabled
    let ttsStream;             // Generated audio MediaStream for WebRTC
    let uploadedAudio;         // Raw uploaded audio data
    let audioBuffer;           // Decoded audio buffer for playback
    let audioSource;           // Web Audio API source node
    let pc1, pc2;             // WebRTC peer connections for loopback
    let micVisualizer, ttsVisualizer;  // Audio visualizer instances
    let mediaRecorder;         // Records microphone for AEC demonstration
    let recordedChunks = [];   // Recorded audio chunks

    // Browser detection for Safari compatibility
    const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
    if (isSafari) {
        console.log('Safari detected - audio playback may be limited');
    }

    // =============================================================================
    // AUDIO FILE HANDLING
    // =============================================================================

    /**
     * Handle user uploaded audio files
     * Loads and decodes audio for later playback through WebRTC
     */
    audioFileInput.addEventListener('change', async (event) => {
        const file = event.target.files[0];
        if (file) {
            try {
                const arrayBuffer = await file.arrayBuffer();
                await loadAudioBuffer(arrayBuffer, file.name);
            } catch (error) {
                showFileError('Error loading audio file');
                console.error('Error loading audio:', error);
            }
        }
    });

    /**
     * Load the OpenAI TTS sample audio
     * Demonstrates the technique with a real TTS voice
     */
    loadSampleButton.addEventListener('click', async () => {
        try {
            fileStatus.textContent = 'Loading TTS sample...';
            fileStatus.style.color = 'blue';
            
            const response = await fetch('https://cdn.openai.com/API/docs/audio/alloy.wav');
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            const arrayBuffer = await response.arrayBuffer();
            await loadAudioBuffer(arrayBuffer, 'TTS Sample');
        } catch (error) {
            showFileError('Error loading sample audio');
            console.error('Error loading sample audio:', error);
        }
    });

    /**
     * Load and decode audio buffer from array buffer
     * @param {ArrayBuffer} arrayBuffer - Raw audio data
     * @param {string} fileName - Display name for the file
     */
    async function loadAudioBuffer(arrayBuffer, fileName) {
        if (audioContext) {
            // Audio context exists - decode immediately
            audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
            fileStatus.textContent = `Loaded: ${fileName} (${audioBuffer.duration.toFixed(1)}s)`;
            fileStatus.style.color = 'green';
            
            // Enable play button if demo is already running
            if (startButton.disabled) {
                playButton.disabled = false;
            }
        } else {
            // Store for later when audio context is created
            uploadedAudio = arrayBuffer;
            fileStatus.textContent = `File ready: ${fileName}`;
            fileStatus.style.color = 'green';
        }
    }

    /**
     * Show file loading error
     * @param {string} message - Error message to display
     */
    function showFileError(message) {
        fileStatus.textContent = message;
        fileStatus.style.color = 'red';
    }

    // =============================================================================
    // MAIN DEMO CONTROL FUNCTIONS
    // =============================================================================

    /**
     * Start the echo cancellation demo
     * Sets up microphone with AEC, audio context, and WebRTC loopback
     */
    async function startDemo() {
        try {
            statusDiv.textContent = 'Requesting microphone access...';
            
            // STEP 1: Get microphone with AEC based on toggle setting
            // This is crucial - echoCancellation: true enables browser's built-in AEC
            const aecEnabled = aecToggle.checked;
            micStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: aecEnabled,  // KEY: Enable/disable AEC based on toggle
                    noiseSuppression: true,
                    autoGainControl: true
                },
                video: false
            });

            statusDiv.textContent = `Setting up audio context and loopback... (AEC: ${aecEnabled ? 'ON' : 'OFF'})`;
            
            // STEP 2: Create Web Audio API context
            audioContext = new AudioContext();

            // Decode uploaded audio if available
            if (uploadedAudio && !audioBuffer) {
                try {
                    audioBuffer = await audioContext.decodeAudioData(uploadedAudio);
                    fileStatus.textContent = `Audio ready: ${audioBuffer.duration.toFixed(1)}s`;
                } catch (error) {
                    showFileError('Error decoding audio');
                    console.error('Error decoding audio:', error);
                }
            }

            // STEP 3: Setup microphone visualization
            micVisualizer = new Visualizer(micStream, micCanvas);
            
            // STEP 4: Start recording microphone to demonstrate AEC effectiveness
            setupRecording();

            // STEP 5: Setup WebRTC peer connection loopback
            // This is the CORE technique - creates the "bridge" for AEC
            setupPeerConnectionLoopback();

            // Initialize WebRTC status indicator
            webrtcStatus.textContent = 'Ready';
            webrtcStatus.style.color = 'green';

            // Update UI state
            startButton.disabled = true;
            playButton.disabled = !audioBuffer;
            stopButton.disabled = false;
            aecToggle.disabled = false;
            
            statusDiv.textContent = audioBuffer ? 
                `Ready. Talk to see your voice, then play the audio. (AEC: ${aecEnabled ? 'ON' : 'OFF'})` : 
                'Please load sample audio or upload an audio file first.';
                
        } catch (err) {
            console.error('Error starting demo:', err);
            statusDiv.textContent = `Error: ${err.name}. Please allow microphone access.`;
        }
    }

    /**
     * Stop the demo and clean up resources
     * Also triggers playback of recorded audio to demonstrate AEC results
     */
    function stopDemo() {
        // Stop audio recording
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
            console.log('Recording stopped');
        }

        // Stop audio playback
        if (audioSource) {
            audioSource.stop();
            audioSource = null;
        }
        
        // Close WebRTC peer connections
        if (pc1) pc1.close();
        if (pc2) pc2.close();
        
        // Stop microphone stream
        if (micStream) {
            micStream.getTracks().forEach(track => track.stop());
        }
        
        // Stop visualizers
        if (micVisualizer) micVisualizer.stop();
        if (ttsVisualizer) ttsVisualizer.stop();
        
        // Reset UI state
        startButton.disabled = false;
        playButton.disabled = true;
        stopButton.disabled = true;
        aecToggle.disabled = false;
        webrtcStatus.textContent = 'Disconnected';
        webrtcStatus.style.color = 'gray';
        statusDiv.textContent = 'Demo stopped. Check the recorded audio below.';
    }

    // =============================================================================
    // WEBRTC LOOPBACK SETUP - THE CORE AEC TECHNIQUE
    // =============================================================================

    /**
     * Setup WebRTC peer connection loopback
     * This is the KEY technique that makes browser AEC work with local audio
     * 
     * How it works:
     * 1. Create two peer connections that connect to each other (pc1 ↔ pc2)
     * 2. When we play local audio, we send it through pc1 → pc2
     * 3. Browser AEC sees this as "remote participant audio" and cancels it
     * 4. This bridges local audio generation to browser's built-in AEC system
     */
    function setupPeerConnectionLoopback() {
        // Create two peer connections for loopback
        pc1 = new RTCPeerConnection();
        pc2 = new RTCPeerConnection();

        console.log('Setting up WebRTC peer connection loopback...');

        // IMPORTANT: ICE candidate exchange for connection establishment
        pc1.onicecandidate = e => {
            if (e.candidate) {
                console.log('PC1 ICE candidate:', e.candidate);
                pc2.addIceCandidate(e.candidate);
            }
        };
        
        pc2.onicecandidate = e => {
            if (e.candidate) {
                console.log('PC2 ICE candidate:', e.candidate);
                pc1.addIceCandidate(e.candidate);
            }
        };

        // Monitor connection states
        pc1.onconnectionstatechange = () => {
            console.log('PC1 connection state:', pc1.connectionState);
            if (pc1.connectionState === 'connected') {
                webrtcStatus.textContent = 'Connected';
                webrtcStatus.style.color = 'green';
            } else if (pc1.connectionState === 'failed') {
                webrtcStatus.textContent = 'Failed';
                webrtcStatus.style.color = 'red';
            }
        };

        pc2.onconnectionstatechange = () => {
            console.log('PC2 connection state:', pc2.connectionState);
        };

        /**
         * CRUCIAL: Handle incoming audio tracks
         * When pc1 sends audio to pc2, this handler receives it
         * The audio gets played through remoteAudio element
         * Browser AEC now "knows" about this audio and can cancel it from microphone
         */
        pc2.ontrack = e => {
            console.log('✅ CRITICAL: Received track from peer connection');
            console.log('Track details:', e.track, e.streams);
            console.log('Track kind:', e.track.kind, 'Track enabled:', e.track.enabled);
            
            if (e.streams && e.streams.length > 0) {
                // Route the received audio to the remoteAudio element
                // This is what makes the browser AEC "aware" of the audio
                remoteAudio.srcObject = e.streams[0];
                console.log('Set remoteAudio source - AEC now knows about this audio');
                console.log('Stream tracks:', e.streams[0].getTracks());
                
                webrtcStatus.textContent = 'Audio Received';
                webrtcStatus.style.color = 'blue';
                
                // Start playback - this is the audio that AEC will cancel
                remoteAudio.play().then(() => {
                    console.log('✅ Remote audio playing - AEC active');
                    webrtcStatus.textContent = 'Playing';
                    webrtcStatus.style.color = 'green';
                }).catch(err => {
                    console.error('Error playing remote audio:', err);
                    webrtcStatus.textContent = 'Play Error';
                    webrtcStatus.style.color = 'red';
                });
            } else {
                console.error('❌ No streams received in ontrack event');
                webrtcStatus.textContent = 'No Audio Stream';
                webrtcStatus.style.color = 'red';
            }
        };

        // WEBRTC HANDSHAKE: Standard offer/answer exchange
        // This establishes the peer connection that will carry our audio
        pc1.createOffer()
            .then(offer => {
                console.log('Created offer:', offer);
                return pc1.setLocalDescription(offer);
            })
            .then(() => {
                console.log('PC1 local description set');
                return pc2.setRemoteDescription(pc1.localDescription);
            })
            .then(() => {
                console.log('PC2 remote description set');
                return pc2.createAnswer();
            })
            .then(answer => {
                console.log('Created answer:', answer);
                return pc2.setLocalDescription(answer);
            })
            .then(() => {
                console.log('PC2 local description set');
                return pc1.setRemoteDescription(pc2.localDescription);
            })
            .then(() => {
                console.log('✅ WebRTC loopback setup completed - ready for AEC');
            })
            .catch(e => {
                console.error("❌ Peer connection setup failed:", e);
                statusDiv.textContent = 'WebRTC setup failed. Echo cancellation may not work properly.';
            });
    }

    // =============================================================================
    // MICROPHONE RECORDING SETUP
    // =============================================================================

    /**
     * Setup microphone recording to demonstrate AEC effectiveness
     * Records the microphone throughout the demo to show before/after AEC processing
     * Handles Safari compatibility for different audio formats
     */
    function setupRecording() {
        recordedChunks = [];
        
        // Safari compatibility: Check supported recording formats
        // Safari prefers MP4 but can record WebM (though playback is limited)
        let options = {};
        if (MediaRecorder.isTypeSupported('audio/mp4')) {
            options.mimeType = 'audio/mp4';
        } else if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
            options.mimeType = 'audio/webm;codecs=opus';
        } else if (MediaRecorder.isTypeSupported('audio/webm')) {
            options.mimeType = 'audio/webm';
        }
        
        console.log('Using MediaRecorder with options:', options);
        mediaRecorder = new MediaRecorder(micStream, options);
        
        // Collect audio data as it's recorded
        mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
                recordedChunks.push(event.data);
            }
        };

        // Handle recording completion
        mediaRecorder.onstop = () => {
            const mimeType = mediaRecorder.mimeType || 'audio/webm';
            const blob = new Blob(recordedChunks, { type: mimeType });
            const url = URL.createObjectURL(blob);
            
            console.log('Recording completed. Blob type:', blob.type, 'Size:', blob.size);
            
            // Setup audio element for playback
            recordedAudio.src = url;
            recordedAudio.load(); // Force reload for better browser compatibility
            
            // Show the recorded audio section
            recordedAudioContainer.style.display = 'block';
            
            // Show Safari compatibility warning if needed
            if (isSafari) {
                safariWarning.style.display = 'block';
            }
            
            // Safari-specific error handling for audio playback
            recordedAudio.onerror = (e) => {
                console.error('Audio playback error:', e);
                console.log('Recorded blob type:', blob.type);
                
                // Show user-friendly error message
                const errorMsg = document.createElement('div');
                errorMsg.style.color = 'orange';
                errorMsg.style.marginTop = '10px';
                errorMsg.innerHTML = '<strong>Playback Issue:</strong> Safari may not support this audio format for playback. Use the download button to save and play in another app.';
                recordedAudioContainer.appendChild(errorMsg);
            };
            
            // Debug logging for audio loading
            recordedAudio.onloadstart = () => console.log('Audio loading started');
            recordedAudio.oncanplay = () => console.log('Audio ready for playback');
            
            // Setup download with proper file extension
            const fileExtension = mimeType.includes('mp4') ? '.m4a' : '.webm';
            downloadButton.onclick = () => {
                const a = document.createElement('a');
                a.href = url;
                a.download = `recorded-audio-aec${fileExtension}`;
                a.click();
            };
        };

        // Start recording immediately when demo begins
        mediaRecorder.start();
        console.log('📹 Started recording microphone for AEC demonstration');
    }

    // =============================================================================
    // AUDIO PLAYBACK THROUGH WEBRTC - THE CORE AEC INTEGRATION
    // =============================================================================

    /**
     * Play audio through WebRTC to trigger echo cancellation
     * This function implements the core technique of routing local audio
     * through WebRTC so browser AEC can recognize and cancel it
     */
    function playAudio() {
        if (!audioContext || !audioBuffer) {
            statusDiv.textContent = 'Please load sample audio or upload an audio file first.';
            return;
        }
        
        // Handle Chrome's autoplay policy - AudioContext must be resumed by user interaction
        if (audioContext.state === 'suspended') {
            audioContext.resume().then(() => {
                console.log('AudioContext resumed after user interaction');
                playAudioInternal();
            });
        } else {
            playAudioInternal();
        }
    }

    /**
     * Internal audio playback function - the heart of the AEC technique
     * 
     * Key steps:
     * 1. Create Web Audio API source from uploaded/sample audio
     * 2. Route audio to MediaStreamDestination (converts to MediaStream)
     * 3. Add MediaStream track to WebRTC peer connection
     * 4. Renegotiate peer connection so audio flows pc1 → pc2 → speakers
     * 5. Browser AEC now "sees" this as remote audio and cancels it from microphone
     */
    function playAudioInternal() {
        statusDiv.textContent = 'Playing audio through WebRTC for echo cancellation...';

        // STEP 1: Create Web Audio API source from our audio buffer
        audioSource = audioContext.createBufferSource();
        audioSource.buffer = audioBuffer;

        // Add gain control for volume management
        const gainNode = audioContext.createGain();
        gainNode.gain.setValueAtTime(0.5, audioContext.currentTime);

        // STEP 2: Convert Web Audio to MediaStream for WebRTC
        // This is crucial - WebRTC can only work with MediaStreams
        const ttsDestination = audioContext.createMediaStreamDestination();
        audioSource.connect(gainNode).connect(ttsDestination);
        
        ttsStream = ttsDestination.stream;
        
        // STEP 3: Setup visualization for the audio being played
        if (ttsVisualizer) ttsVisualizer.stop();
        ttsVisualizer = new Visualizer(ttsStream, ttsCanvas);

        // STEP 4: THE MAGIC - Add audio track to WebRTC peer connection
        // This makes browser AEC "aware" of our locally generated audio
        const ttsTrack = ttsStream.getAudioTracks()[0];
        console.log('🎵 Adding local audio track to WebRTC peer connection');
        console.log('Track details:', ttsTrack);
        pc1.addTrack(ttsTrack, ttsStream);

        // STEP 5: Renegotiate WebRTC connection to include the new audio track
        // This is required when adding tracks after initial connection setup
        webrtcStatus.textContent = 'Renegotiating...';
        webrtcStatus.style.color = 'orange';

        console.log('🔄 Renegotiating WebRTC connection for new audio track...');
        pc1.createOffer()
            .then(offer => {
                console.log('Created renegotiation offer:', offer);
                return pc1.setLocalDescription(offer);
            })
            .then(() => {
                console.log('PC1 local description updated');
                return pc2.setRemoteDescription(pc1.localDescription);
            })
            .then(() => {
                console.log('PC2 remote description updated');
                return pc2.createAnswer();
            })
            .then(answer => {
                console.log('Created renegotiation answer:', answer);
                return pc2.setLocalDescription(answer);
            })
            .then(() => {
                console.log('PC2 local description updated');
                return pc1.setRemoteDescription(pc2.localDescription);
            })
            .then(() => {
                console.log('✅ WebRTC renegotiation completed successfully');
                webrtcStatus.textContent = 'Ready for Audio';
                webrtcStatus.style.color = 'blue';
                
                // STEP 6: Start audio playback after WebRTC is ready
                audioSource.start();
                console.log('🎵 Audio playback started - AEC should now be active');
                playButton.disabled = true;
            })
            .catch(e => {
                console.error("❌ WebRTC renegotiation failed:", e);
                webrtcStatus.textContent = 'Renegotiation Failed';
                webrtcStatus.style.color = 'red';
                
                // Fallback: start audio anyway (AEC might not work properly)
                audioSource.start();
                console.log('⚠️ Audio started without successful WebRTC renegotiation');
                playButton.disabled = true;
            });

        // Handle audio playback completion
        audioSource.onended = () => {
            console.log('🎵 Audio playback completed');
            playButton.disabled = false;
            if (ttsVisualizer) ttsVisualizer.stop();
            webrtcStatus.textContent = 'Ready';
            webrtcStatus.style.color = 'green';
        };
    }

    // =============================================================================
    // AEC TOGGLE FUNCTIONALITY
    // =============================================================================

    /**
     * Handle AEC toggle changes
     * Allows real-time switching between echo cancellation ON/OFF
     * Demonstrates the dramatic difference AEC makes
     */
    aecToggle.addEventListener('change', async () => {
        const aecEnabled = aecToggle.checked;
        
        // Update UI indicators
        if (aecEnabled) {
            aecStatus.textContent = 'ON';
            aecStatus.style.color = 'green';
            micVisualizerTitle.textContent = 'Microphone Input (AEC ON)';
        } else {
            aecStatus.textContent = 'OFF';
            aecStatus.style.color = 'red';
            micVisualizerTitle.textContent = 'Microphone Input (AEC OFF)';
        }
        
        // If demo is running, restart microphone with new AEC setting
        // This requires getting a new MediaStream with updated constraints
        if (!startButton.disabled) {
            await restartMicrophoneStream();
        }
    });

    /**
     * Restart microphone stream with updated AEC settings
     * This is necessary because AEC settings are applied when creating the MediaStream
     * and cannot be changed dynamically - we need a new stream
     */
    async function restartMicrophoneStream() {
        try {
            const aecEnabled = aecToggle.checked;
            statusDiv.textContent = `Updating microphone... (AEC: ${aecEnabled ? 'ON' : 'OFF'})`;
            
            // Stop current microphone stream and visualizer
            if (micStream) {
                micStream.getTracks().forEach(track => track.stop());
            }
            if (micVisualizer) {
                micVisualizer.stop();
            }
            
            // Stop current recording (will restart with new stream)
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
            
            // Get new microphone stream with updated AEC setting
            micStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: aecEnabled,  // Apply new AEC setting
                    noiseSuppression: true,
                    autoGainControl: true
                },
                video: false
            });
            
            // Restart visualization and recording with new stream
            micVisualizer = new Visualizer(micStream, micCanvas);
            setupRecording();
            
            statusDiv.textContent = `Microphone updated. (AEC: ${aecEnabled ? 'ON' : 'OFF'}) - ${audioBuffer ? 'Ready to play audio!' : 'Load audio first.'}`;
            console.log(`🎙️ Microphone restarted with AEC: ${aecEnabled ? 'ON' : 'OFF'}`);
        } catch (error) {
            console.error('Error restarting microphone:', error);
            statusDiv.textContent = 'Error updating microphone settings';
        }
    }

    // =============================================================================
    // AUDIO VISUALIZATION CLASS
    // =============================================================================

    /**
     * Real-time audio visualizer using Web Audio API
     * Shows waveform visualization for microphone input and generated audio
     * Helps users see the audio activity and AEC effects visually
     */
    class Visualizer {
        constructor(stream, canvasElement) {
            this.stream = stream;
            this.canvas = canvasElement;
            this.canvasCtx = this.canvas.getContext('2d');
            this.animationFrameId = null;
            
            // Create separate AudioContext for analysis (to avoid conflicts)
            this.audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            this.analyser = this.audioCtx.createAnalyser();
            this.analyser.fftSize = 2048;
            
            // Connect MediaStream to analyser
            this.source = this.audioCtx.createMediaStreamSource(this.stream);
            this.source.connect(this.analyser);

            // Setup data arrays for visualization
            this.bufferLength = this.analyser.frequencyBinCount;
            this.dataArray = new Uint8Array(this.bufferLength);

            // Start the visualization loop
            this.draw();
        }

        /**
         * Animation loop for real-time waveform visualization
         * Continuously draws the audio waveform on canvas
         */
        draw() {
            this.animationFrameId = requestAnimationFrame(() => this.draw());
            
            // Get current audio data
            this.analyser.getByteTimeDomainData(this.dataArray);

            // Clear and setup canvas
            this.canvasCtx.fillStyle = '#2c3e50';
            this.canvasCtx.fillRect(0, 0, this.canvas.width, this.canvas.height);
            this.canvasCtx.lineWidth = 2;
            this.canvasCtx.strokeStyle = '#00c6ff';
            this.canvasCtx.beginPath();

            // Draw waveform
            const sliceWidth = this.canvas.width * 1.0 / this.bufferLength;
            let x = 0;

            for (let i = 0; i < this.bufferLength; i++) {
                const v = this.dataArray[i] / 128.0;
                const y = v * this.canvas.height / 2;

                if (i === 0) {
                    this.canvasCtx.moveTo(x, y);
                } else {
                    this.canvasCtx.lineTo(x, y);
                }
                x += sliceWidth;
            }
            
            this.canvasCtx.lineTo(this.canvas.width, this.canvas.height / 2);
            this.canvasCtx.stroke();
        }

        /**
         * Clean up visualizer resources
         * Prevents memory leaks and "AudioContext already closed" errors
         */
        stop() {
            if (this.animationFrameId) {
                cancelAnimationFrame(this.animationFrameId);
            }
            if (this.source) {
                this.source.disconnect();
            }
            // Safe AudioContext cleanup for Safari compatibility
            if (this.audioCtx && this.audioCtx.state !== 'closed') {
                try {
                    this.audioCtx.close();
                } catch (error) {
                    console.log('AudioContext already closed or closing');
                }
            }
        }
    }

    // =============================================================================
    // EVENT LISTENERS - WIRE UP THE UI
    // =============================================================================
    startButton.addEventListener('click', startDemo);
    playButton.addEventListener('click', playAudio);
    stopButton.addEventListener('click', stopDemo);

</script>
</body>
</html>