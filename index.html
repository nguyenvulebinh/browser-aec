<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Demo: Echo Cancellation</title>
    <style>
        body { font-family: sans-serif; background-color: #f4f4f9; color: #333; line-height: 1.6; padding: 20px; }
        .container { max-width: 800px; margin: 0 auto; background: white; padding: 25px; border-radius: 8px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); }
        h1, h2 { text-align: center; color: #444; }
        .controls { text-align: center; margin-bottom: 20px; }
        button { font-size: 16px; padding: 10px 20px; margin: 5px; cursor: pointer; border: none; border-radius: 5px; background-color: #007bff; color: white; transition: background-color 0.2s; }
        button:disabled { background-color: #ccc; cursor: not-allowed; }
        button:hover:not(:disabled) { background-color: #0056b3; }
        .file-input-container { text-align: center; margin: 15px 0; }
        input[type="file"] { margin: 10px; padding: 8px; border: 2px solid #007bff; border-radius: 4px; }
        .visualizers { display: flex; justify-content: space-around; flex-wrap: wrap; margin-top: 20px; }
        .visualizer-box { text-align: center; margin: 10px; }
        canvas { background-color: #2c3e50; border-radius: 4px; }
        .instructions { background-color: #eaf2ff; border-left: 5px solid #007bff; padding: 15px; margin-top: 20px; border-radius: 4px; }
        .recorded-audio { text-align: center; margin-top: 20px; padding: 15px; background-color: #f0f8ff; border-radius: 4px; }
        p, li { font-size: 16px; }
        #status { text-align: center; font-weight: bold; margin-top: 15px; height: 20px; }
        audio { margin: 10px; }
    </style>
</head>
<body>

<div class="container">
    <h1>Echo Cancellation</h1>
    <p class="instructions">
        <strong>How This Demo Works:</strong><br><br>
        
        This demonstration showcases browser-based <strong>Acoustic Echo Cancellation (AEC)</strong> using a WebRTC loopback technique. Here's the technical breakdown:
        
        <br><br><strong>The Problem:</strong><br>
        When a voice assistant plays audio (like TTS responses), that audio can be picked up by the microphone and interfere with voice recognition. Browsers have built-in AEC, but it only works with audio that arrives through WebRTC connections - not locally generated audio.
        
        <br><br><strong>getUserMedia vs WebRTC - The Missing Link:</strong><br>
        <strong>getUserMedia's Role:</strong> Provides the microphone stream with AEC capability built-in. When you set <code>echoCancellation: true</code>, the browser applies acoustic echo cancellation to the microphone audio stream.<br><br>
        
        <strong>The Key Insight:</strong> Browser AEC only cancels audio it "knows about" - specifically audio that comes through WebRTC peer connections.<br><br>
        
        <strong>Without WebRTC:</strong><br>
        &nbsp;&nbsp;&nbsp;Locally Generated Audio â†’ Speakers â†’ Microphone â†’ getUserMedia<br>
        &nbsp;&nbsp;&nbsp;Browser AEC: "I don't know about this audio, so I won't cancel it"<br><br>
        
        <strong>With WebRTC Loopback:</strong><br>
        &nbsp;&nbsp;&nbsp;Locally Generated Audio â†’ WebRTC â†’ Speakers â†’ Microphone â†’ getUserMedia<br>
        &nbsp;&nbsp;&nbsp;Browser AEC: "This is 'remote participant' audio, I should cancel it!"
        
        <br><br><strong>The Solution - WebRTC Loopback:</strong><br>
        1. <strong>Audio Generation:</strong> Audio is created using the Web Audio API (either uploaded files or the TTS sample)<br>
        2. <strong>WebRTC Routing:</strong> The audio is routed through a peer connection loopback:<br>
        &nbsp;&nbsp;&nbsp;â€¢ Audio â†’ WebRTC peer connection â†’ "WebRTC Echo Audio" â†’ Your speakers<br>
        &nbsp;&nbsp;&nbsp;â€¢ Same audio stream â†’ AEC system (as reference signal)<br>
        3. <strong>AEC:</strong> The browser's AEC now "knows" about the audio and automatically removes it from the microphone input
        
        <br><br><strong>The Loopback Trick:</strong><br>
        We create two peer connections (pc1 â†” pc2) talking to each other. The TTS audio is sent through this loopback, making the browser treat our locally generated audio as if it were a "remote participant" in a video call. This bridges locally generated audio to the browser's built-in AEC system.
        
        <br><br><strong>Real-time Visualization:</strong><br>
        â€¢ <strong>Microphone Input:</strong> Shows your voice and background audio after AEC processing<br>
        â€¢ <strong>Audio to be Cancelled:</strong> Shows the reference audio that should be removed<br>
        â€¢ <strong>WebRTC Echo Audio:</strong> The audio stream that teaches AEC what to cancel
        
        <br><br><strong>AEC Toggle:</strong><br>
        Toggle echo cancellation ON/OFF to see the dramatic difference. With AEC OFF, played audio bleeds into the microphone. With AEC ON, it's completely removed while preserving your voice.
        
        <br><br><strong>Recording:</strong><br>
        The demo continuously records your microphone to demonstrate how well AEC works in practice. When you stop, you can hear the processed audio that a voice assistant would actually receive.
        
        <br><br>This technique is used by real voice assistants to prevent their own speech from triggering unwanted activations or interfering with user commands.
    </p>

    <div class="file-input-container">
        <button id="loadSampleButton">Load TTS Sample</button>
        <div style="margin: 10px 0;">OR</div>
        <input type="file" id="audioFileInput" accept="audio/*">
        <div id="fileStatus"></div>
    </div>

    <div class="controls">
        <div style="margin-bottom: 15px;">
            <label style="font-size: 16px; font-weight: bold;">
                <input type="checkbox" id="aecToggle" checked style="margin-right: 8px; transform: scale(1.2);">
                Echo Cancellation (AEC) 
                <span id="aecStatus" style="color: green;">ON</span>
            </label>
        </div>
        <button id="startButton">Start Demo</button>
        <button id="playButton" disabled>Play Audio</button>
        <button id="stopButton" disabled>Stop Demo</button>
    </div>

    <div id="status"></div>

    <div style="text-align: center; margin: 10px 0;">
        <label style="font-weight: bold;">WebRTC Echo Audio (for AEC): <span id="webrtcStatus" style="color: orange;">Connecting...</span></label><br>
        <audio id="remoteAudio" autoplay playsinline controls style="margin: 10px 0;"></audio>
    </div>

    <div class="visualizers">
        <div class="visualizer-box">
            <h2 id="micVisualizerTitle">Microphone Input (AEC ON)</h2>
            <canvas id="micCanvas" width="300" height="150"></canvas>
        </div>
        <div class="visualizer-box">
            <h2>Audio to be Cancelled</h2>
            <canvas id="ttsCanvas" width="300" height="150"></canvas>
        </div>
    </div>

    <div class="recorded-audio" id="recordedAudioContainer" style="display:none;">
        <h2>Recorded Microphone Audio (After AEC)</h2>
        <div id="safariWarning" style="display:none; background-color: #fff3cd; border: 1px solid #ffecb5; padding: 10px; margin: 10px 0; border-radius: 4px; color: #856404;">
            <strong>Safari Note:</strong> Safari may not play back recorded audio directly. If playback doesn't work, use the download button to save the audio file and play it in another app.
        </div>
        <audio id="recordedAudio" controls></audio>
        <br>
        <button id="downloadButton">Download Recording</button>
    </div>
</div>

<script>
    // =============================================================================
    // DOM ELEMENT REFERENCES
    // =============================================================================
    const startButton = document.getElementById('startButton');
    const playButton = document.getElementById('playButton');
    const stopButton = document.getElementById('stopButton');
    const audioFileInput = document.getElementById('audioFileInput');
    const fileStatus = document.getElementById('fileStatus');
    const remoteAudio = document.getElementById('remoteAudio');
    const statusDiv = document.getElementById('status');
    const micCanvas = document.getElementById('micCanvas');
    const ttsCanvas = document.getElementById('ttsCanvas');
    const recordedAudioContainer = document.getElementById('recordedAudioContainer');
    const recordedAudio = document.getElementById('recordedAudio');
    const downloadButton = document.getElementById('downloadButton');
    const loadSampleButton = document.getElementById('loadSampleButton');
    const aecToggle = document.getElementById('aecToggle');
    const aecStatus = document.getElementById('aecStatus');
    const micVisualizerTitle = document.getElementById('micVisualizerTitle');
    const webrtcStatus = document.getElementById('webrtcStatus');
    const safariWarning = document.getElementById('safariWarning');

    // =============================================================================
    // GLOBAL VARIABLES
    // =============================================================================
    let audioContext;           // Web Audio API context
    let micStream;             // Microphone MediaStream with AEC enabled
    let ttsStream;             // Generated audio MediaStream for WebRTC
    let uploadedAudio;         // Raw uploaded audio data
    let audioBuffer;           // Decoded audio buffer for playback
    let audioSource;           // Web Audio API source node
    let pc1, pc2;             // WebRTC peer connections for loopback
    let micVisualizer, ttsVisualizer;  // Audio visualizer instances
    let mediaRecorder;         // Records microphone for AEC demonstration
    let recordedChunks = [];   // Recorded audio chunks

    // Browser detection for Safari compatibility
    const isSafari = /^((?!chrome|android).)*safari/i.test(navigator.userAgent);
    if (isSafari) {
        console.log('Safari detected - audio playback may be limited');
    }

    // =============================================================================
    // AUDIO FILE HANDLING
    // =============================================================================

    /**
     * Handle user uploaded audio files
     * Loads and decodes audio for later playback through WebRTC
     */
    audioFileInput.addEventListener('change', async (event) => {
        const file = event.target.files[0];
        if (file) {
            try {
                const arrayBuffer = await file.arrayBuffer();
                await loadAudioBuffer(arrayBuffer, file.name);
            } catch (error) {
                showFileError('Error loading audio file');
                console.error('Error loading audio:', error);
            }
        }
    });

    /**
     * Load the OpenAI TTS sample audio
     * Demonstrates the technique with a real TTS voice
     */
    loadSampleButton.addEventListener('click', async () => {
        try {
            fileStatus.textContent = 'Loading TTS sample...';
            fileStatus.style.color = 'blue';
            
            const response = await fetch('https://cdn.openai.com/API/docs/audio/alloy.wav');
            if (!response.ok) {
                throw new Error(`HTTP error! status: ${response.status}`);
            }
            
            const arrayBuffer = await response.arrayBuffer();
            await loadAudioBuffer(arrayBuffer, 'TTS Sample');
        } catch (error) {
            showFileError('Error loading sample audio');
            console.error('Error loading sample audio:', error);
        }
    });

    /**
     * Load and decode audio buffer from array buffer
     * @param {ArrayBuffer} arrayBuffer - Raw audio data
     * @param {string} fileName - Display name for the file
     */
    async function loadAudioBuffer(arrayBuffer, fileName) {
        if (audioContext) {
            // Audio context exists - decode immediately
            audioBuffer = await audioContext.decodeAudioData(arrayBuffer);
            fileStatus.textContent = `Loaded: ${fileName} (${audioBuffer.duration.toFixed(1)}s)`;
            fileStatus.style.color = 'green';
            
            // Enable play button if demo is already running
            if (startButton.disabled) {
                playButton.disabled = false;
            }
        } else {
            // Store for later when audio context is created
            uploadedAudio = arrayBuffer;
            fileStatus.textContent = `File ready: ${fileName}`;
            fileStatus.style.color = 'green';
        }
    }

    /**
     * Show file loading error
     * @param {string} message - Error message to display
     */
    function showFileError(message) {
        fileStatus.textContent = message;
        fileStatus.style.color = 'red';
    }

    // =============================================================================
    // MAIN DEMO CONTROL FUNCTIONS
    // =============================================================================

    /**
     * Start the echo cancellation demo
     * Sets up microphone with AEC, audio context, and WebRTC loopback
     */
    async function startDemo() {
        try {
            statusDiv.textContent = 'Requesting microphone access...';
            
            // STEP 1: Get microphone with AEC based on toggle setting
            // This is crucial - echoCancellation: true enables browser's built-in AEC
            const aecEnabled = aecToggle.checked;
            micStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: aecEnabled,  // KEY: Enable/disable AEC based on toggle
                    noiseSuppression: true,
                    autoGainControl: true
                },
                video: false
            });

            statusDiv.textContent = `Setting up audio context and loopback... (AEC: ${aecEnabled ? 'ON' : 'OFF'})`;
            
            // STEP 2: Create Web Audio API context
            audioContext = new AudioContext();

            // Decode uploaded audio if available
            if (uploadedAudio && !audioBuffer) {
                try {
                    audioBuffer = await audioContext.decodeAudioData(uploadedAudio);
                    fileStatus.textContent = `Audio ready: ${audioBuffer.duration.toFixed(1)}s`;
                } catch (error) {
                    showFileError('Error decoding audio');
                    console.error('Error decoding audio:', error);
                }
            }

            // STEP 3: Setup microphone visualization
            micVisualizer = new Visualizer(micStream, micCanvas);
            
            // STEP 4: Start recording microphone to demonstrate AEC effectiveness
            setupRecording();

            // STEP 5: Setup WebRTC peer connection loopback
            // This is the CORE technique - creates the "bridge" for AEC
            setupPeerConnectionLoopback();

            // Initialize WebRTC status indicator
            webrtcStatus.textContent = 'Ready';
            webrtcStatus.style.color = 'green';

            // Update UI state
            startButton.disabled = true;
            playButton.disabled = !audioBuffer;
            stopButton.disabled = false;
            aecToggle.disabled = false;
            
            statusDiv.textContent = audioBuffer ? 
                `Ready. Talk to see your voice, then play the audio. (AEC: ${aecEnabled ? 'ON' : 'OFF'})` : 
                'Please load sample audio or upload an audio file first.';
                
        } catch (err) {
            console.error('Error starting demo:', err);
            statusDiv.textContent = `Error: ${err.name}. Please allow microphone access.`;
        }
    }

    /**
     * Stop the demo and clean up resources
     * Also triggers playback of recorded audio to demonstrate AEC results
     */
    function stopDemo() {
        // Stop audio recording
        if (mediaRecorder && mediaRecorder.state !== 'inactive') {
            mediaRecorder.stop();
            console.log('Recording stopped');
        }

        // Stop audio playback
        if (audioSource) {
            audioSource.stop();
            audioSource = null;
        }
        
        // Close WebRTC peer connections
        if (pc1) pc1.close();
        if (pc2) pc2.close();
        
        // Stop microphone stream
        if (micStream) {
            micStream.getTracks().forEach(track => track.stop());
        }
        
        // Stop visualizers
        if (micVisualizer) micVisualizer.stop();
        if (ttsVisualizer) ttsVisualizer.stop();
        
        // Reset UI state
        startButton.disabled = false;
        playButton.disabled = true;
        stopButton.disabled = true;
        aecToggle.disabled = false;
        webrtcStatus.textContent = 'Disconnected';
        webrtcStatus.style.color = 'gray';
        statusDiv.textContent = 'Demo stopped. Check the recorded audio below.';
    }

    // =============================================================================
    // WEBRTC LOOPBACK SETUP - THE CORE AEC TECHNIQUE
    // =============================================================================

    /**
     * Setup WebRTC peer connection loopback
     * This is the KEY technique that makes browser AEC work with local audio
     * 
     * How it works:
     * 1. Create two peer connections that connect to each other (pc1 â†” pc2)
     * 2. When we play local audio, we send it through pc1 â†’ pc2
     * 3. Browser AEC sees this as "remote participant audio" and cancels it
     * 4. This bridges local audio generation to browser's built-in AEC system
     */
    function setupPeerConnectionLoopback() {
        // Create two peer connections for loopback
        pc1 = new RTCPeerConnection();
        pc2 = new RTCPeerConnection();

        console.log('Setting up WebRTC peer connection loopback...');

        // IMPORTANT: ICE candidate exchange for connection establishment
        pc1.onicecandidate = e => {
            if (e.candidate) {
                console.log('PC1 ICE candidate:', e.candidate);
                pc2.addIceCandidate(e.candidate);
            }
        };
        
        pc2.onicecandidate = e => {
            if (e.candidate) {
                console.log('PC2 ICE candidate:', e.candidate);
                pc1.addIceCandidate(e.candidate);
            }
        };

        // Monitor connection states
        pc1.onconnectionstatechange = () => {
            console.log('PC1 connection state:', pc1.connectionState);
            if (pc1.connectionState === 'connected') {
                webrtcStatus.textContent = 'Connected';
                webrtcStatus.style.color = 'green';
            } else if (pc1.connectionState === 'failed') {
                webrtcStatus.textContent = 'Failed';
                webrtcStatus.style.color = 'red';
            }
        };

        pc2.onconnectionstatechange = () => {
            console.log('PC2 connection state:', pc2.connectionState);
        };

        /**
         * CRUCIAL: Handle incoming audio tracks
         * When pc1 sends audio to pc2, this handler receives it
         * The audio gets played through remoteAudio element
         * Browser AEC now "knows" about this audio and can cancel it from microphone
         */
        pc2.ontrack = e => {
            console.log('âœ… CRITICAL: Received track from peer connection');
            console.log('Track details:', e.track, e.streams);
            console.log('Track kind:', e.track.kind, 'Track enabled:', e.track.enabled);
            
            if (e.streams && e.streams.length > 0) {
                // Route the received audio to the remoteAudio element
                // This is what makes the browser AEC "aware" of the audio
                remoteAudio.srcObject = e.streams[0];
                console.log('Set remoteAudio source - AEC now knows about this audio');
                console.log('Stream tracks:', e.streams[0].getTracks());
                
                webrtcStatus.textContent = 'Audio Received';
                webrtcStatus.style.color = 'blue';
                
                // Start playback - this is the audio that AEC will cancel
                remoteAudio.play().then(() => {
                    console.log('âœ… Remote audio playing - AEC active');
                    webrtcStatus.textContent = 'Playing';
                    webrtcStatus.style.color = 'green';
                }).catch(err => {
                    console.error('Error playing remote audio:', err);
                    webrtcStatus.textContent = 'Play Error';
                    webrtcStatus.style.color = 'red';
                });
            } else {
                console.error('âŒ No streams received in ontrack event');
                webrtcStatus.textContent = 'No Audio Stream';
                webrtcStatus.style.color = 'red';
            }
        };

        // WEBRTC HANDSHAKE: Standard offer/answer exchange
        // This establishes the peer connection that will carry our audio
        pc1.createOffer()
            .then(offer => {
                console.log('Created offer:', offer);
                return pc1.setLocalDescription(offer);
            })
            .then(() => {
                console.log('PC1 local description set');
                return pc2.setRemoteDescription(pc1.localDescription);
            })
            .then(() => {
                console.log('PC2 remote description set');
                return pc2.createAnswer();
            })
            .then(answer => {
                console.log('Created answer:', answer);
                return pc2.setLocalDescription(answer);
            })
            .then(() => {
                console.log('PC2 local description set');
                return pc1.setRemoteDescription(pc2.localDescription);
            })
            .then(() => {
                console.log('âœ… WebRTC loopback setup completed - ready for AEC');
            })
            .catch(e => {
                console.error("âŒ Peer connection setup failed:", e);
                statusDiv.textContent = 'WebRTC setup failed. Echo cancellation may not work properly.';
            });
    }

    // =============================================================================
    // MICROPHONE RECORDING SETUP
    // =============================================================================

    /**
     * Setup microphone recording to demonstrate AEC effectiveness
     * Records the microphone throughout the demo to show before/after AEC processing
     * Handles Safari compatibility for different audio formats
     */
    function setupRecording() {
        recordedChunks = [];
        
        // Safari compatibility: Check supported recording formats
        // Safari prefers MP4 but can record WebM (though playback is limited)
        let options = {};
        if (MediaRecorder.isTypeSupported('audio/mp4')) {
            options.mimeType = 'audio/mp4';
        } else if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
            options.mimeType = 'audio/webm;codecs=opus';
        } else if (MediaRecorder.isTypeSupported('audio/webm')) {
            options.mimeType = 'audio/webm';
        }
        
        console.log('Using MediaRecorder with options:', options);
        mediaRecorder = new MediaRecorder(micStream, options);
        
        // Collect audio data as it's recorded
        mediaRecorder.ondataavailable = (event) => {
            if (event.data.size > 0) {
                recordedChunks.push(event.data);
            }
        };

        // Handle recording completion
        mediaRecorder.onstop = () => {
            const mimeType = mediaRecorder.mimeType || 'audio/webm';
            const blob = new Blob(recordedChunks, { type: mimeType });
            const url = URL.createObjectURL(blob);
            
            console.log('Recording completed. Blob type:', blob.type, 'Size:', blob.size);
            
            // Setup audio element for playback
            recordedAudio.src = url;
            recordedAudio.load(); // Force reload for better browser compatibility
            
            // Show the recorded audio section
            recordedAudioContainer.style.display = 'block';
            
            // Show Safari compatibility warning if needed
            if (isSafari) {
                safariWarning.style.display = 'block';
            }
            
            // Safari-specific error handling for audio playback
            recordedAudio.onerror = (e) => {
                console.error('Audio playback error:', e);
                console.log('Recorded blob type:', blob.type);
                
                // Show user-friendly error message
                const errorMsg = document.createElement('div');
                errorMsg.style.color = 'orange';
                errorMsg.style.marginTop = '10px';
                errorMsg.innerHTML = '<strong>Playback Issue:</strong> Safari may not support this audio format for playback. Use the download button to save and play in another app.';
                recordedAudioContainer.appendChild(errorMsg);
            };
            
            // Debug logging for audio loading
            recordedAudio.onloadstart = () => console.log('Audio loading started');
            recordedAudio.oncanplay = () => console.log('Audio ready for playback');
            
            // Setup download with proper file extension
            const fileExtension = mimeType.includes('mp4') ? '.m4a' : '.webm';
            downloadButton.onclick = () => {
                const a = document.createElement('a');
                a.href = url;
                a.download = `recorded-audio-aec${fileExtension}`;
                a.click();
            };
        };

        // Start recording immediately when demo begins
        mediaRecorder.start();
        console.log('ðŸ“¹ Started recording microphone for AEC demonstration');
    }

    // =============================================================================
    // AUDIO PLAYBACK THROUGH WEBRTC - THE CORE AEC INTEGRATION
    // =============================================================================

    /**
     * Play audio through WebRTC to trigger echo cancellation
     * This function implements the core technique of routing local audio
     * through WebRTC so browser AEC can recognize and cancel it
     */
    function playAudio() {
        if (!audioContext || !audioBuffer) {
            statusDiv.textContent = 'Please load sample audio or upload an audio file first.';
            return;
        }
        
        // Handle Chrome's autoplay policy - AudioContext must be resumed by user interaction
        if (audioContext.state === 'suspended') {
            audioContext.resume().then(() => {
                console.log('AudioContext resumed after user interaction');
                playAudioInternal();
            });
        } else {
            playAudioInternal();
        }
    }

    /**
     * Internal audio playback function - the heart of the AEC technique
     * 
     * Key steps:
     * 1. Create Web Audio API source from uploaded/sample audio
     * 2. Route audio to MediaStreamDestination (converts to MediaStream)
     * 3. Add MediaStream track to WebRTC peer connection
     * 4. Renegotiate peer connection so audio flows pc1 â†’ pc2 â†’ speakers
     * 5. Browser AEC now "sees" this as remote audio and cancels it from microphone
     */
    function playAudioInternal() {
        statusDiv.textContent = 'Playing audio through WebRTC for echo cancellation...';

        // STEP 1: Create Web Audio API source from our audio buffer
        audioSource = audioContext.createBufferSource();
        audioSource.buffer = audioBuffer;

        // Add gain control for volume management
        const gainNode = audioContext.createGain();
        gainNode.gain.setValueAtTime(0.5, audioContext.currentTime);

        // STEP 2: Convert Web Audio to MediaStream for WebRTC
        // This is crucial - WebRTC can only work with MediaStreams
        const ttsDestination = audioContext.createMediaStreamDestination();
        audioSource.connect(gainNode).connect(ttsDestination);
        
        ttsStream = ttsDestination.stream;
        
        // STEP 3: Setup visualization for the audio being played
        if (ttsVisualizer) ttsVisualizer.stop();
        ttsVisualizer = new Visualizer(ttsStream, ttsCanvas);

        // STEP 4: THE MAGIC - Add audio track to WebRTC peer connection
        // This makes browser AEC "aware" of our locally generated audio
        const ttsTrack = ttsStream.getAudioTracks()[0];
        console.log('ðŸŽµ Adding local audio track to WebRTC peer connection');
        console.log('Track details:', ttsTrack);
        pc1.addTrack(ttsTrack, ttsStream);

        // STEP 5: Renegotiate WebRTC connection to include the new audio track
        // This is required when adding tracks after initial connection setup
        webrtcStatus.textContent = 'Renegotiating...';
        webrtcStatus.style.color = 'orange';

        console.log('ðŸ”„ Renegotiating WebRTC connection for new audio track...');
        pc1.createOffer()
            .then(offer => {
                console.log('Created renegotiation offer:', offer);
                return pc1.setLocalDescription(offer);
            })
            .then(() => {
                console.log('PC1 local description updated');
                return pc2.setRemoteDescription(pc1.localDescription);
            })
            .then(() => {
                console.log('PC2 remote description updated');
                return pc2.createAnswer();
            })
            .then(answer => {
                console.log('Created renegotiation answer:', answer);
                return pc2.setLocalDescription(answer);
            })
            .then(() => {
                console.log('PC2 local description updated');
                return pc1.setRemoteDescription(pc2.localDescription);
            })
            .then(() => {
                console.log('âœ… WebRTC renegotiation completed successfully');
                webrtcStatus.textContent = 'Ready for Audio';
                webrtcStatus.style.color = 'blue';
                
                // STEP 6: Start audio playback after WebRTC is ready
                audioSource.start();
                console.log('ðŸŽµ Audio playback started - AEC should now be active');
                playButton.disabled = true;
            })
            .catch(e => {
                console.error("âŒ WebRTC renegotiation failed:", e);
                webrtcStatus.textContent = 'Renegotiation Failed';
                webrtcStatus.style.color = 'red';
                
                // Fallback: start audio anyway (AEC might not work properly)
                audioSource.start();
                console.log('âš ï¸ Audio started without successful WebRTC renegotiation');
                playButton.disabled = true;
            });

        // Handle audio playback completion
        audioSource.onended = () => {
            console.log('ðŸŽµ Audio playback completed');
            playButton.disabled = false;
            if (ttsVisualizer) ttsVisualizer.stop();
            webrtcStatus.textContent = 'Ready';
            webrtcStatus.style.color = 'green';
        };
    }

    // =============================================================================
    // AEC TOGGLE FUNCTIONALITY
    // =============================================================================

    /**
     * Handle AEC toggle changes
     * Allows real-time switching between echo cancellation ON/OFF
     * Demonstrates the dramatic difference AEC makes
     */
    aecToggle.addEventListener('change', async () => {
        const aecEnabled = aecToggle.checked;
        
        // Update UI indicators
        if (aecEnabled) {
            aecStatus.textContent = 'ON';
            aecStatus.style.color = 'green';
            micVisualizerTitle.textContent = 'Microphone Input (AEC ON)';
        } else {
            aecStatus.textContent = 'OFF';
            aecStatus.style.color = 'red';
            micVisualizerTitle.textContent = 'Microphone Input (AEC OFF)';
        }
        
        // If demo is running, restart microphone with new AEC setting
        // This requires getting a new MediaStream with updated constraints
        if (!startButton.disabled) {
            await restartMicrophoneStream();
        }
    });

    /**
     * Restart microphone stream with updated AEC settings
     * This is necessary because AEC settings are applied when creating the MediaStream
     * and cannot be changed dynamically - we need a new stream
     */
    async function restartMicrophoneStream() {
        try {
            const aecEnabled = aecToggle.checked;
            statusDiv.textContent = `Updating microphone... (AEC: ${aecEnabled ? 'ON' : 'OFF'})`;
            
            // Stop current microphone stream and visualizer
            if (micStream) {
                micStream.getTracks().forEach(track => track.stop());
            }
            if (micVisualizer) {
                micVisualizer.stop();
            }
            
            // Stop current recording (will restart with new stream)
            if (mediaRecorder && mediaRecorder.state !== 'inactive') {
                mediaRecorder.stop();
            }
            
            // Get new microphone stream with updated AEC setting
            micStream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: aecEnabled,  // Apply new AEC setting
                    noiseSuppression: true,
                    autoGainControl: true
                },
                video: false
            });
            
            // Restart visualization and recording with new stream
            micVisualizer = new Visualizer(micStream, micCanvas);
            setupRecording();
            
            statusDiv.textContent = `Microphone updated. (AEC: ${aecEnabled ? 'ON' : 'OFF'}) - ${audioBuffer ? 'Ready to play audio!' : 'Load audio first.'}`;
            console.log(`ðŸŽ™ï¸ Microphone restarted with AEC: ${aecEnabled ? 'ON' : 'OFF'}`);
        } catch (error) {
            console.error('Error restarting microphone:', error);
            statusDiv.textContent = 'Error updating microphone settings';
        }
    }

    // =============================================================================
    // AUDIO VISUALIZATION CLASS
    // =============================================================================

    /**
     * Real-time audio visualizer using Web Audio API
     * Shows waveform visualization for microphone input and generated audio
     * Helps users see the audio activity and AEC effects visually
     */
    class Visualizer {
        constructor(stream, canvasElement) {
            this.stream = stream;
            this.canvas = canvasElement;
            this.canvasCtx = this.canvas.getContext('2d');
            this.animationFrameId = null;
            
            // Create separate AudioContext for analysis (to avoid conflicts)
            this.audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            this.analyser = this.audioCtx.createAnalyser();
            this.analyser.fftSize = 2048;
            
            // Connect MediaStream to analyser
            this.source = this.audioCtx.createMediaStreamSource(this.stream);
            this.source.connect(this.analyser);

            // Setup data arrays for visualization
            this.bufferLength = this.analyser.frequencyBinCount;
            this.dataArray = new Uint8Array(this.bufferLength);

            // Start the visualization loop
            this.draw();
        }

        /**
         * Animation loop for real-time waveform visualization
         * Continuously draws the audio waveform on canvas
         */
        draw() {
            this.animationFrameId = requestAnimationFrame(() => this.draw());
            
            // Get current audio data
            this.analyser.getByteTimeDomainData(this.dataArray);

            // Clear and setup canvas
            this.canvasCtx.fillStyle = '#2c3e50';
            this.canvasCtx.fillRect(0, 0, this.canvas.width, this.canvas.height);
            this.canvasCtx.lineWidth = 2;
            this.canvasCtx.strokeStyle = '#00c6ff';
            this.canvasCtx.beginPath();

            // Draw waveform
            const sliceWidth = this.canvas.width * 1.0 / this.bufferLength;
            let x = 0;

            for (let i = 0; i < this.bufferLength; i++) {
                const v = this.dataArray[i] / 128.0;
                const y = v * this.canvas.height / 2;

                if (i === 0) {
                    this.canvasCtx.moveTo(x, y);
                } else {
                    this.canvasCtx.lineTo(x, y);
                }
                x += sliceWidth;
            }
            
            this.canvasCtx.lineTo(this.canvas.width, this.canvas.height / 2);
            this.canvasCtx.stroke();
        }

        /**
         * Clean up visualizer resources
         * Prevents memory leaks and "AudioContext already closed" errors
         */
        stop() {
            if (this.animationFrameId) {
                cancelAnimationFrame(this.animationFrameId);
            }
            if (this.source) {
                this.source.disconnect();
            }
            // Safe AudioContext cleanup for Safari compatibility
            if (this.audioCtx && this.audioCtx.state !== 'closed') {
                try {
                    this.audioCtx.close();
                } catch (error) {
                    console.log('AudioContext already closed or closing');
                }
            }
        }
    }

    // =============================================================================
    // EVENT LISTENERS - WIRE UP THE UI
    // =============================================================================
    startButton.addEventListener('click', startDemo);
    playButton.addEventListener('click', playAudio);
    stopButton.addEventListener('click', stopDemo);

</script>
</body>
</html>